# vae-anomaly-detection
1. Project Overview

This project demonstrates:

Implementation of the VAE architecture

Encoder â†’ Î¼ and log(ÏƒÂ²)

Reparameterization trick

Decoder

ELBO loss (Reconstruction + KL Divergence)

Training the VAE only on normal samples

Computing reconstruction errors to detect anomalies

Training a baseline Autoencoder (AE) with similar architecture

Comparing both models using AUC-ROC scores

This tests understanding of:

Generative modeling

Probabilistic deep learning

2. Dataset Information

Dataset used: Fashion-MNIST
Each image is:

28 Ã— 28 pixels

Grayscale

Flattened to 784-dimensional vector in code

Normal Class (Training Data)

Class 0: T-shirt/Top

Anomaly Classes (Testing Data)

Classes 1 to 9

The model learns the distribution of class 0 and flags other classes based on reconstruction error.

3. Variational Autoencoder (VAE) â€” Mathematical Foundation
3.1 Evidence Lower Bound (ELBO)

The VAE optimizes:

ğ¿
=
ğ¸
ğ‘
(
ğ‘§
âˆ£
ğ‘¥
)
[
log
â¡
ğ‘
(
ğ‘¥
âˆ£
ğ‘§
)
]
âˆ’
ğ¾
ğ¿
(
ğ‘
(
ğ‘§
âˆ£
ğ‘¥
)
â€…â€Š
âˆ£
âˆ£
â€…â€Š
ğ‘
(
ğ‘§
)
)
L=E
q(zâˆ£x)
	â€‹

[logp(xâˆ£z)]âˆ’KL(q(zâˆ£x)âˆ£âˆ£p(z))

Rewriting as a loss to minimize:

Loss
=
Reconstruction Loss
+
ğ›½
â‹…
ğ¾
ğ¿
Loss=Reconstruction Loss+Î²â‹…KL

Where Î² = 1.0 is a standard VAE.

Optimization

Unsupervised anomaly detection
